{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5AzNo1pOhIcH"
   },
   "source": [
    "The implementation of Naive Bayes and Logistic Regression is supposed to be from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "36YSVA-j5xQ5"
   },
   "source": [
    "# Naive Bayes (50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zGGm0CAthrVm"
   },
   "source": [
    "Naive Bayes methods are a set of supervised learning algorithms based on applying Bayes’ theorem with the “naive” assumption of conditional independence between every pair of features given the value of the class variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rr5nM6pCigHq"
   },
   "source": [
    "$P(y \\mid x_1, \\dots, x_n) = \\frac{P(y) P(x_1, \\dots, x_n \\mid y)} {P(x_1, \\dots, x_n)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iBCcOp2Zis3f"
   },
   "source": [
    "$P(x_i | y, x_1, \\dots, x_{i-1}, x_{i+1}, \\dots, x_n) = P(x_i | y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5_8sGl2JiyC1"
   },
   "source": [
    "$P(y \\mid x_1, \\dots, x_n) = \\frac{P(y) \\prod_{i=1}^{n} P(x_i \\mid y)}{P(x_1, \\dots, x_n)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6cm2XNKHi2PG"
   },
   "source": [
    "$\\begin{align}\\begin{aligned}P(y \\mid x_1, \\dots, x_n) \\propto P(y) \\prod_{i=1}{n} P(x_i \\mid y)\\\\\\Downarrow\\\\\\hat{y} = \\arg\\max_y P(y) \\prod_{i=1}^{n} P(x_i \\mid y)\\end{aligned}\\end{align}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I8hTshtjjD4c"
   },
   "source": [
    "In spite of their apparently over-simplified assumptions, naive Bayes classifiers have worked quite well in many real-world situations, famously document classification and spam filtering. They require a small amount of training data to estimate the necessary parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MhGOePaPjPbG"
   },
   "source": [
    "Naive Bayes learners and classifiers can be extremely fast compared to more sophisticated methods. The decoupling of the class conditional feature distributions means that each distribution can be independently estimated as a one dimensional distribution. This in turn helps to alleviate problems stemming from the curse of dimensionality.\n",
    "\n",
    "On the flip side, although naive Bayes is known as a decent classifier, it is known to be a bad estimator, so the probability outputs are not to be taken too seriously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "luSQvKVFiMVR"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mG9eriVCk5ZW"
   },
   "source": [
    "### Dataset\n",
    "Load the given dataset. The last column contains the labels. \n",
    "\n",
    "Preprocess if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "mcK58W-tkxxK"
   },
   "outputs": [],
   "source": [
    "#TODO: Load the .txt file\n",
    "data = ... # Numpy array format\n",
    "labels = ... # Numpy array format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SS4_sX-Dmd9H"
   },
   "source": [
    "Consider the values of each class. Create a dictionary for the dataset, with classes as keys and the entries of the dataset as values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "0nDpA3InmdKG"
   },
   "outputs": [],
   "source": [
    "def create_class_dictionary(data, labels):\n",
    "  #TODO\n",
    "  pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7oRTA9MPnMhW"
   },
   "source": [
    "For the dataset dictionary, find the mean and standard deviation of all classes. The output format should be a list of two lists, the first one the mean and standard deviation of the first column and the second one is for the second column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "i4WGyr1Kl69o"
   },
   "outputs": [],
   "source": [
    "def info(data):\n",
    "  #TODO\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "bD_VHsMpnhg_"
   },
   "outputs": [],
   "source": [
    "def class_info(class_dictionary):\n",
    "  #TODO: call the info function to return the mean and standard deviation of each column for each class\n",
    "  pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LWp9wJ7LpIK6"
   },
   "source": [
    "### Visualization\n",
    "Use the imported libraries to visualize the given data. \n",
    "\n",
    "Why is the info step valid in this dataset? \n",
    "\n",
    "What is the type of this dataset's distribution? With other distribution types, what action would be needed to obtain the mean and standard variation info?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mdut2HsDpG7P"
   },
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w9YhQPq5qQmi"
   },
   "source": [
    "### Model Details\n",
    "\n",
    "As explained above, to create this model, you need a prior function and a likelihood function.\n",
    "\n",
    "In the likelihood function, you need to calculate the probability of the query belonging to a class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "DE_8cfT9qT7F"
   },
   "outputs": [],
   "source": [
    "def prior(class_dictionary, labels):\n",
    "  #TODO\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "BrZwF9jNq54e"
   },
   "outputs": [],
   "source": [
    "def likelihood(class_dictionary, query):\n",
    "  #TODO\n",
    "  pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yyc87dyz0ZtC"
   },
   "source": [
    "### Predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "LC7-G7-C0UeI"
   },
   "outputs": [],
   "source": [
    "def predict(data, labels, query):\n",
    "  #TODO\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "cRfNQZ8-04DL"
   },
   "outputs": [],
   "source": [
    "def NB(data, labels, queries):\n",
    "  #TODO: call the predict function for all queries\n",
    "  pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ptYF5MxR1IXj"
   },
   "source": [
    "### Test\n",
    "To test the model, import a suitable dataset from sklearn library to check the accuracy of your model. Then import GaussianNB from sklearn and compare your model's result with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Pq-bDAf11HKI"
   },
   "outputs": [],
   "source": [
    "def train_test_split(data, labels, test_size):\n",
    "  #TODO return X_train, X_test, y_train, y_test\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "ogw89sLN1o1_"
   },
   "outputs": [],
   "source": [
    "def accuracy(ground_truth, predictions):\n",
    "  # call the NB function for your chosen dataset\n",
    "  # calculate accuracy\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "BGwM37ZX1vBk"
   },
   "outputs": [],
   "source": [
    "# compare with GaussianNB from sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pB3_dUtp2RGb"
   },
   "source": [
    "# Linear Regression (35)\n",
    "\n",
    "Linear regression analysis is used to predict the value of a variable based on the value of another variable. The variable you want to predict is called the dependent variable. The variable you are using to predict the other variable's value is called the independent variable.\n",
    "\n",
    "Ordinary least squares Linear Regression.\n",
    "\n",
    "LinearRegression from sklearn.linear_model fits a linear model with coefficients w = (w1, …, wp) to minimize the residual sum of squares between the observed targets in the dataset, and the targets predicted by the linear approximation.\n",
    "\n",
    "In this section, you will implement a simple linear regression model using sklearn. Only the first feature of the diabetes dataset is required for this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "V6nHfZLH2D_1"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "from sklearn import datasets, linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "VsJA9bM235h9"
   },
   "outputs": [],
   "source": [
    "diabetes_X, diabetes_y = datasets.load_diabetes(return_X_y=True)\n",
    "diabetes_X = ... # Change this so that your model will use only one feature (arbitary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ynGlCxqV4R5n"
   },
   "source": [
    "Split the data into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "VeN0-ObT4Nu7"
   },
   "outputs": [],
   "source": [
    "# you can use the train_test_split(data, labels, test_size) function from the previous section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E0diHAiL4qww"
   },
   "source": [
    "Create the model using sklearn. Then train it using the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_76nVOTH4lLI"
   },
   "outputs": [],
   "source": [
    "model = ... #TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jogQMqjc41q9"
   },
   "source": [
    "Make predictions for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LIIMboiY5CiT"
   },
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mv3PUcdS5Dlo"
   },
   "source": [
    "Visualize your predictions and compare them to ground truth using the imported libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LsTXKZvM5KWp"
   },
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d8Kg5i465SNb"
   },
   "source": [
    "# Logistic Regression (15 + 50)\n",
    "\n",
    "This type of statistical model (also known as logit model) is often used for classification and predictive analytics. Logistic regression estimates the probability of an event occurring, such as voted or didn't vote, based on a given dataset of independent variables.\n",
    "\n",
    "$S(h(x)) = \\frac{1}{1 + e^{-(\\theta_0 + \\theta_1 x_1 + \\theta_1 x_2 + \\cdots + \\theta_n x_n})} = \\frac{1}{1 + e^{-\\theta^T x}} \\tag{2}$\n",
    "\n",
    "The sigmoid function is of importance here and is defined as:\n",
    "\n",
    "$S(x) = \\frac{1}{1 + e^{-x}}$\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V_Rc3pUF6Pji"
   },
   "source": [
    "Calculate the sigmoid function and visualize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "v_bGZ_X85f4W"
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "  #TODO\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "Qrcgiip56e50"
   },
   "outputs": [],
   "source": [
    "#TODO: visualize the sigmoid function for arbitary range of x. you can use np.linspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IwL9DuGh7EHh"
   },
   "source": [
    "#Dataset\n",
    "Load the given dataset (same as naive bayes). Add a new column at the end of the dataset containing only 1s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "D00oSB3Q6rD9"
   },
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ejeLlynP7Xa_"
   },
   "source": [
    "### Predictions\n",
    "\n",
    "Simply implement the math above to make predictions. Since we are using numbers here, define the threshold of 0.5 for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "IJrCamvm7WzK"
   },
   "outputs": [],
   "source": [
    "def predict(weights, x):\n",
    "  #TODO\n",
    "  pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b-XmjNsO75bS"
   },
   "source": [
    "### Loss Function\n",
    "Cross-entropy loss, or log loss, measures the performance of a classification model whose output is a probability value between 0 and 1. Cross-entropy loss increases as the predicted probability diverges from the actual label. So predicting a probability of .012 when the actual observation label is 1 would be bad and result in a high loss value. A perfect model would have a log loss of 0.\n",
    "\n",
    "In binary classification, where the number of classes M\n",
    " equals 2, cross-entropy can be calculated as:\n",
    "\n",
    "$−(ylog(p)+(1−y)log(1−p))$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "iM3SUJA-717R"
   },
   "outputs": [],
   "source": [
    "def cross_entropy(y_true, y_pred):\n",
    "  #TODO: calculate cross entropy using the formula above\n",
    "  pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sgOxchuk89lL"
   },
   "source": [
    "### Gradient Descent\n",
    "\n",
    "Gradient Descent is an optimization algorithm for finding a local minimum of a differentiable function. Gradient descent in machine learning is simply used to find the values of a function's parameters (coefficients) that minimize a cost function as far as possible.\n",
    "\n",
    "$\\theta := \\theta - \\alpha \\nabla_\\theta H \\tag{6}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "75qw2i3680QK"
   },
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, weight, num_of_epochs, learning_rate = 0.005):\n",
    "  #TODO: calculate gradient descent\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "vrNePqPL-SDM"
   },
   "outputs": [],
   "source": [
    "def LR(train_set, labels, test_set, num_of_epochs, learning_rate = 0.005):\n",
    "  #TODO: simply gather all the functions you already implemented in this section to make valid and complete predictions for a given dataset\n",
    "  pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O6P0Usic-Maj"
   },
   "source": [
    "### Test\n",
    "\n",
    "To test the model, import a suitable dataset from sklearn library to check the accuracy of your model. Then import LogisticRegression from sklearn and compare your model's result with it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "11f8e3rS_CbB"
   },
   "outputs": [],
   "source": [
    "def accuracy(ground_truth, predictions):\n",
    "  # call the LR function for your chosen dataset\n",
    "  # calculate accuracy\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "opX7Phbj_JN-"
   },
   "outputs": [],
   "source": [
    "# compare with sklearn.linear_model.LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GGu4MOmU_QVq"
   },
   "source": [
    "### Visualization\n",
    "\n",
    "During your model's training, save the accuracy and loss of each epoch, and then plot them using the imported libraries. Explain the pattern. If the result is not satisfactory, change the learning rate, num of epochs, initial weights, etc and observe their effects on the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EdlupQWb_3OI"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
