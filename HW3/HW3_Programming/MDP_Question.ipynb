{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Decision Process"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Introduction\n",
    "\n",
    "In this section, we will introduce the concepts of MDP, Q-values, and V-values. These concepts are fundamental to the field of AI and machine learning, as they are used to model **decision-making problems** in various domains such as \"robotics\", \"finance\", and \"healthcare\".\n",
    "\n",
    "MDP stands for Markov Decision Process. It is a mathematical framework for modeling decision-making problems in which the outcomes are partly random and partly under the control of a decision-maker. MDPs are defined by a set of states, a set of actions, a reward function, and a transition function. The goal is to find a policy that maximizes the expected cumulative reward over time.\n",
    "\n",
    "Q-values and V-values are two important concepts in the context of MDPs. A Q-value represents the expected cumulative reward of taking a particular action in a particular state and following a specific policy thereafter. A V-value represents the expected cumulative reward of being in a particular state and following a specific policy thereafter. These values are used to evaluate and improve the policy of an agent in an MDP."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: The Basics of MDPs\n",
    "\n",
    "In this section, we will explain the basic components of an MDP.\n",
    "\n",
    "An MDP is defined by \"a set of states\", \"a set of actions\", \"a reward function\", and \"a transition function\". The state space is the set of all possible states that the agent can be in. The action space is the set of all possible actions that the agent can take. The reward function defines the reward the agent receives for each action taken in a particular state. The transition function defines the probability of moving from one state to another state after taking a particular action.\n",
    "\n",
    "To illustrate these concepts, let's consider an example of a **robot that needs to navigate through a maze**. The robot can be in one of several states, such as at the start of the maze, at a junction in the maze, or at the end of the maze. This robot takes an action. With Probability of **0.8** It goes in that desired direction but with probability of **0.2** It goes in the perpendicular direction (0.1, 0.1 for each)!\n",
    "\n",
    "In an MDP, the agent interacts with the environment by selecting actions based on its current state and the expected future reward. The goal of the agent is to find a policy that maximizes the expected cumulative reward over time."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION**\n",
    "\n",
    "1. What are the state space, action space, reward function, and transition function of the robot in the maze example? Explain why you think each of these components is important for the robot to navigate through the maze.\n",
    "\n",
    "2. Is our environment stochastic or deterministic? Why?!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define The MDP**:\n",
    "\n",
    "Based on your choice of rewards and transitions and the state space, define the MDP for the robot in the maze example. You can complete the following code to define the MDP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our Maze is: \n",
      " [[2 0 0 0 0]\n",
      " [0 1 1 0 1]\n",
      " [0 0 0 0 0]\n",
      " [0 1 1 1 3]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Definition of the maze\n",
    "maze = np.array([[2, 0, 0, 0, 0],\n",
    "                 [0, 1, 1, 0, 1],\n",
    "                 [0, 0, 0, 0, 0],\n",
    "                 [0, 1, 1, 1, 3]])\n",
    "\n",
    "print(\"Our Maze is: \\n\", maze)\n",
    "\n",
    "# Define the states and actions\n",
    "states = np.arange(np.prod(maze.shape))\n",
    "actions = ['UP', 'DOWN', 'LEFT', 'RIGHT'] \n",
    "\n",
    "# Define the reward function\n",
    "rewards = np.zeros(maze.shape)   \n",
    "rewards[maze == 1] = -10  # assign negative rewards for hitting walls\n",
    "rewards[maze == 3] = 10 # assign positive rewards for reaching the goal\n",
    "rewards[maze == 0] = -1 # assign negative rewards for each step\n",
    "\n",
    "\n",
    "# Set the discount factor\n",
    "discount = 0.9\n",
    "\n",
    "# Define the initial value function\n",
    "values = np.zeros(maze.shape)\n",
    "values[maze == 3] = 100  \n",
    "values[maze == 2] = -10\n",
    "# values[maze == 2] = -2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Computing V-values\n",
    "\n",
    "In this section, we will explain how to compute V-values for an MDP using the Bellman equation.\n",
    "\n",
    "The Bellman equation is a recursive equation that expresses the value of a state in terms of the values of its successor states. It is defined as:\n",
    "\n",
    "$$V(s) = R(s) + \\gamma * \\max_a (\\sum_{s'} P(s, a, s') * V(s'))$$\n",
    "\n",
    "where V(s) is the value of state s, R(s) is the reward for being in state s, Î³ is the discount factor that determines the importance of future rewards, P(s, a, s') is the probability of moving from state s to state s' after taking action a, and max_a is the maximum over all possible actions a.\n",
    "\n",
    "To compute the V-values for an MDP, we start with an initial estimate of the V-values and update them iteratively using the Bellman equation until they converge to the true values. The update rule is:\n",
    "\n",
    "$$V(s) \\leftarrow R(s) + \\gamma * \\max_a (\\sum_{s'} P(s, a, s') * V(s'))$$\n",
    "\n",
    "We can use dynamic programming algorithms such as value iteration or policy iteration to compute the V-values.\n",
    "\n",
    "We can use the Bellman equation to compute the V-values for each state in the maze. The V-values represent the expected cumulative reward that the robot can obtain if it starts from that state and follows an optimal policy thereafter. Complete the code below:\n",
    "\n",
    "(**Note:** your final result can be slightly different from the result printed below and it's okay!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-10.          16.40583497  24.62761654  35.01675448  -3.86813187]\n",
      " [ 16.30333962   0.           0.          47.4289986    0.        ]\n",
      " [ 24.50320696  31.34865814  47.4289986   69.76262157  93.60287974]\n",
      " [ 16.30333962   0.           0.           0.         100.        ]]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Compute the V-values using Bellman equations\n",
    "...\n",
    "\n",
    "\n",
    "def get_next_value(state, action,discount):\n",
    "    \"\"\"\n",
    "    Returns the next state and reward given the current state and action.\n",
    "    Assumes deterministic actions.\n",
    "    \"\"\"\n",
    "    next_state = state\n",
    "    n = maze.shape[0]\n",
    "    m = maze.shape[1]\n",
    "    row, col = state\n",
    "    rev = rewards[maze == 1][0]\n",
    "    if action == 'UP' :\n",
    "        perp1 = rev\n",
    "        perp1_val = values[state]\n",
    "        perp2 = rev\n",
    "        perp2_val = values[state]\n",
    "        right_dir = rev \n",
    "        right_val = values[state]\n",
    "        if col > 0 and col < m- 1 :\n",
    "            perp1 = rewards[row,col - 1]\n",
    "            perp2 = rewards[row,col + 1]\n",
    "            perp1_val = values[row,col-1]\n",
    "            perp2_val = values[row,col+1]\n",
    "        elif col == m - 1 :\n",
    "            perp1 = rewards[row,col - 1]\n",
    "            perp1_val = values[row,col -1]\n",
    "        elif col == 0 :\n",
    "            perp2 = rewards[row,col + 1]\n",
    "            perp2_val = values[row,col +1]\n",
    "        if row > 0 :\n",
    "            right_dir = rewards[row-1,col]\n",
    "            right_val = values[row - 1,col] \n",
    "        reward = 0.8 * (right_dir + discount * right_val) + 0.1 * (perp1 + discount * perp1_val) + 0.1 * (perp2 + discount * perp2_val)\n",
    "    elif action == 'DOWN' :\n",
    "        perp1 = rev\n",
    "        perp1_val = values[state]\n",
    "        perp2 = rev\n",
    "        perp2_val = values[state]\n",
    "        right_dir = rev \n",
    "        right_val = values[state]\n",
    "        if col > 0 and col < m - 1 :\n",
    "            perp1 = rewards[row,col - 1]\n",
    "            perp2 = rewards[row,col + 1]\n",
    "            perp1_val = values[row,col-1]\n",
    "            perp2_val = values[row,col+1]\n",
    "        elif col == m - 1 :\n",
    "            perp1 = rewards[row,col - 1]\n",
    "            perp1_val = values[row,col -1]\n",
    "        elif col == 0 :\n",
    "            perp2 = rewards[row,col + 1]\n",
    "            perp2_val = values[row,col +1]\n",
    "        if row < n -1 :\n",
    "            right_dir = rewards[row+1,col] \n",
    "            right_val = values[row+1,col]\n",
    "        reward = 0.8 * (right_dir + discount * right_val) + 0.1 * (perp1 + discount * perp1_val) + 0.1 * (perp2 + discount * perp2_val)\n",
    "    elif action == 'LEFT' : \n",
    "        perp1 = rev\n",
    "        perp1_val = values[state]\n",
    "        perp2 = rev\n",
    "        perp2_val = values[state]\n",
    "        right_dir = rev \n",
    "        right_val = values[state]\n",
    "        if row > 0 and row < n - 1 :\n",
    "            perp1 = rewards[row - 1, col]\n",
    "            perp2 = rewards[row + 1, col]\n",
    "            perp1_val = values[row - 1,col]\n",
    "            perp2_val = values[row + 1,col]\n",
    "        elif row == n - 1 :\n",
    "            perp1 = rewards[row - 1, col]\n",
    "            perp1_val = values[row - 1,col]\n",
    "        elif row == 0 :\n",
    "            perp2 = rewards[row + 1, col]\n",
    "            perp2_val = values[row + 1,col]\n",
    "        if col > 0 :\n",
    "            right_dir = rewards[row, col - 1]\n",
    "            right_val = rewards[row, col - 1] \n",
    "        reward = 0.8 * (right_dir + discount * right_val) + 0.1 * (perp1 + discount * perp1_val) + 0.1 * (perp2 + discount * perp2_val)\n",
    "    elif action == 'RIGHT' : \n",
    "        perp1 = rev\n",
    "        perp1_val = values[state]\n",
    "        perp2 = rev\n",
    "        perp2_val = values[state]\n",
    "        right_dir = rev \n",
    "        right_val = values[state] \n",
    "        if row > 0 and row < n - 1 :\n",
    "            perp1 = rewards[row - 1, col]\n",
    "            perp2 = rewards[row + 1, col]\n",
    "            perp1_val = values[row - 1,col]\n",
    "            perp2_val = values[row + 1,col]\n",
    "        elif row == n - 1 :\n",
    "            perp1 = rewards[row - 1, col]\n",
    "            perp1_val = values[row - 1,col]\n",
    "        elif row == 0 :\n",
    "            perp2 = rewards[row + 1, col]\n",
    "            perp2_val = values[row + 1,col]\n",
    "        if col < m - 1  :\n",
    "            right_dir = rewards[row, col + 1] \n",
    "            right_val = values[row, col+1]\n",
    "        reward = 0.8 * (right_dir + discount * right_val) + 0.1 * (perp1 + discount * perp1_val) + 0.1 * (perp2 + discount * perp2_val)\n",
    "    return reward    \n",
    "\n",
    "n = maze.shape[0]\n",
    "m = maze.shape[1]\n",
    "num_of_itters = 15\n",
    "epsilon = 0.001\n",
    "for itter in range(num_of_itters):\n",
    "    delta = 0\n",
    "    old_value = np.copy(values)\n",
    "    for row in range(n):\n",
    "        for col in range(m):\n",
    "            state = (row,col)\n",
    "            new_value = 0\n",
    "            v = []\n",
    "            if maze[row,col] == 1 :\n",
    "                old_value[row,col] = 0\n",
    "                continue\n",
    "            elif maze[row,col] == 3 :\n",
    "                continue\n",
    "            elif maze[row,col] == 2:\n",
    "                continue\n",
    "            for action in actions:\n",
    "                value = get_next_value(state, action,discount)\n",
    "                # print(value)\n",
    "                v.append(value)\n",
    "            old_value[row,col] = max(v)\n",
    "    values = np.copy(old_value)\n",
    "print(values)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Computing Q-values\n",
    "\n",
    "In this section, we will explain how to compute Q-values for an MDP using the Bellman equation.\n",
    "\n",
    "The Q-values represent the expected cumulative reward that the robot can obtain if it starts from a particular state and takes a particular action, and then follows an optimal policy thereafter. The Q-values can be computed using the Bellman equation as follows:\n",
    "\n",
    "$$Q(s, a) = R(s, a) + \\gamma * \\sum_{s'} (P(s, a, s') * \\max_{a'} (Q(s', a')))$$\n",
    "\n",
    "where Q(s, a) is the Q-value of state-action pair (s, a), R(s, a) is the reward for taking action a in state s, Î³ is the discount factor that determines the importance of future rewards, P(s, a, s') is the probability of moving from state s to state s' after taking action a, max_a' is the maximum over all possible actions a' in state s', and sum_s' is the sum over all possible successor states s' of state s.\n",
    "\n",
    "To compute the Q-values for an MDP, we start with an initial estimate of the Q-values and update them iteratively using the Bellman equation until they converge to the true values. The update rule is:\n",
    "\n",
    "$$Q(s, a) \\leftarrow R(s, a) + \\gamma * \\sum_{s'} (P(s, a, s') * \\max_{a'} (Q(s', a')))$$\n",
    "\n",
    "We can use dynamic programming algorithms such as Q-learning or SARSA to compute the Q-values.\n",
    "\n",
    "\n",
    "We can use the Q-learning algorithm to compute the Q-values for each state-action pair in the maze. The Q-values represent the expected cumulative reward that the robot can obtain if it starts from a particular state and takes a particular action, and then follows an optimal policy thereafter. Complete the code below:\n",
    "\n",
    "(**Note:** your final result can be slightly different from the result printed below and it's okay!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[  0.           0.           0.           0.        ]\n",
      "  [  5.02868667  -6.78351451  -0.52347485  16.40840906]\n",
      "  [ 14.15991696  -3.57196695  -1.30351451  24.62854871]\n",
      "  [ 18.88041685  35.01723262   4.80011778   2.73506283]\n",
      "  [ -9.08167891  -6.29662397  -3.86813187 -13.13318681]]\n",
      "\n",
      " [[ -7.73269943  16.30960958   4.94369316  -6.79471137]\n",
      "  [  0.           0.           0.           0.        ]\n",
      "  [  0.           0.           0.           0.        ]\n",
      "  [ 22.41206322  47.42908753  -5.96985616   1.23014384]\n",
      "  [  0.           0.           0.           0.        ]]\n",
      "\n",
      " [[ 14.86507239  14.86507239  12.37691014  24.505635  ]\n",
      "  [ -1.7261015   -1.7261015   -3.52        31.34887899]\n",
      "  [  0.90001517   0.90001517  -3.52        47.42908753]\n",
      "  [ 45.84174805   4.49286905   1.64860987  69.76268329]\n",
      "  [  5.60289512  93.60289512   7.48        68.39407341]]\n",
      "\n",
      " [[ 16.30960958   3.2057051    6.31099372  -5.42741081]\n",
      "  [  0.           0.           0.           0.        ]\n",
      "  [  0.           0.           0.           0.        ]\n",
      "  [  0.           0.           0.           0.        ]\n",
      "  [  0.           0.           0.           0.        ]]]\n"
     ]
    }
   ],
   "source": [
    "#TODO: Compute Q-Values using Bellman equations\n",
    "...\n",
    "q_values = np.zeros((*maze.shape, len(actions)))\n",
    "for itter in range(num_of_itters):\n",
    "    delta = 0\n",
    "    old_q_values = np.copy(q_values)\n",
    "    for row in range(n):\n",
    "        for col in range(m):\n",
    "            state = (row, col)\n",
    "            if maze[row, col] == 1:\n",
    "                old_q_values[row, col, :] = 0\n",
    "                continue\n",
    "            elif maze[row, col] in (2, 3):\n",
    "                continue\n",
    "            for action_idx, action in enumerate(actions):\n",
    "                value = get_next_value(state, action, discount)\n",
    "                old_q_values[row, col, action_idx] = value\n",
    "                delta = max(delta, abs(q_values[row, col, action_idx] - value))\n",
    "    q_values = np.copy(old_q_values)\n",
    "print(q_values)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Visualizing the Optimal Policy\n",
    "\n",
    "Now that we have computed the Q-values, we can use them to find the optimal policy, which is the sequence of actions that the robot should take in each state to maximize its expected reward. We can visualize the optimal policy as arrows in a grid, where each arrow corresponds to the action with the highest Q-value in the corresponding state. Complete the code below:\n",
    "\n",
    "(**Note:** your final result can be slightly different from the result printed below and it's okay!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['UP', 'DOWN', 'LEFT', 'RIGHT']\n",
      "[[0 3 3 1 2]\n",
      " [1 0 0 1 0]\n",
      " [3 3 3 3 1]\n",
      " [0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Compute the optimal policy\n",
    "\n",
    "policy = np.argmax(q_values, axis=-1)\n",
    "\n",
    "print(actions)\n",
    "print(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['â' 'â' 'â' 'â' 'â']\n",
      " ['â' 'â' 'â' 'â' 'â']\n",
      " ['â' 'â' 'â' 'â' 'â']\n",
      " ['â' 'â' 'â' 'â' 'â']]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#TODO: Visualize optimal policy as arrows\n",
    "# Replace numbers with arrow characters\n",
    "arrow_arr = np.full_like(policy, '', dtype=np.dtype('<U2'))\n",
    "arrow_arr[policy == 0] = 'â'\n",
    "arrow_arr[policy == 1] = 'â'\n",
    "arrow_arr[policy == 2] = 'â'\n",
    "arrow_arr[policy == 3] = 'â'\n",
    "print(arrow_arr)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(2,2))\n",
    "ax.axis('off')\n",
    "ax.invert_yaxis()\n",
    "for (i, j), arrow in np.ndenumerate(arrow_arr):\n",
    "    ax.text(j, i, arrow, ha='center', va='center', fontsize=20)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#TODO: Show the maze. show start state (S) in red, goal state(G) in green, obstacles(X) in white, and Path (P) in white\n",
    "\n",
    "maze_in_chars = np.full_like(maze, '', dtype=np.dtype('<U2'))\n",
    "maze_in_chars[maze == 0] = 'P'\n",
    "maze_in_chars[maze == 1] = 'X'\n",
    "maze_in_chars[maze == 2] = 'S'\n",
    "maze_in_chars[maze == 3] = 'G'\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(2, 2),facecolor='black')\n",
    "ax.axis('off')\n",
    "ax.invert_yaxis()\n",
    "\n",
    "for (i, j), arrow in np.ndenumerate(maze_in_chars):\n",
    "    text_obj = ax.text(j, i, arrow, ha='center', va='center', fontsize=20)\n",
    "    if arrow == 'S':\n",
    "        text_obj.set_color('red')\n",
    "    elif arrow == 'X' or arrow == 'P':\n",
    "        text_obj.set_color('white')\n",
    "    elif arrow == 'G':\n",
    "        text_obj.set_color('green')\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
